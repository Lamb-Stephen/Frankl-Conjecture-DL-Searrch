{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport itertools\nfrom collections import Counter\nimport multiprocessing as mp\nimport time\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nclass FranklAdversarialSearch:\n    def __init__(self, ground_set_size=6, max_iterations=10000, population_size=100, \n                 tournament_size=5, mutation_rate=0.3, crossover_rate=0.7, \n                 early_stopping=1000, use_multiprocessing=True, verbose=True):\n        self.ground_set_size = ground_set_size\n        self.max_iterations = max_iterations\n        self.population_size = population_size\n        self.tournament_size = tournament_size\n        self.mutation_rate = mutation_rate\n        self.crossover_rate = crossover_rate\n        self.early_stopping = early_stopping\n        self.use_multiprocessing = use_multiprocessing\n        self.verbose = verbose\n        \n        # Results tracking\n        self.best_family = None\n        self.best_max_frequency = 1.0  # Start with worst case (all elements in all sets)\n        self.history = {\n            'best_frequencies': [],\n            'avg_frequencies': [],\n            'family_sizes': [],\n            'diversity': []\n        }\n        \n        # Caches to improve performance\n        self._union_cache = {}\n        self._frequency_cache = {}\n        self._metrics_cache = {}\n    \n    def is_union_closed(self, family):\n        \"\"\"Check if a family of sets is union-closed.\"\"\"\n        # Use a set for O(1) lookups\n        family_set = set(family)\n        \n        for set1, set2 in itertools.combinations(family, 2):\n            # Create a cache key for this pair\n            cache_key = (set1, set2)\n            if cache_key in self._union_cache:\n                union = self._union_cache[cache_key]\n            else:\n                union = tuple(sorted(set(set1).union(set(set2))))\n                self._union_cache[cache_key] = union\n                \n            if union not in family_set:\n                return False\n        return True\n    \n    def get_union_closure(self, seed_sets):\n        \"\"\"Generate the union closure of a collection of sets.\"\"\"\n        if not seed_sets:\n            return []\n            \n        # Convert sets to frozensets for hashability\n        closure = set(frozenset(s) for s in seed_sets if s)  # Skip empty sets\n        \n        # Add the empty set if it's in seed_sets\n        if () in seed_sets:\n            closure.add(frozenset())\n            \n        old_size = 0\n        \n        # Keep adding unions until no new sets are formed\n        while old_size != len(closure):\n            old_size = len(closure)\n            new_sets = set()\n            \n            # Use a list to avoid modifying the set during iteration\n            closure_list = list(closure)\n            for i in range(len(closure_list)):\n                for j in range(i, len(closure_list)):\n                    set1 = closure_list[i]\n                    set2 = closure_list[j]\n                    \n                    # Use cache for performance\n                    cache_key = (frozenset(set1), frozenset(set2))\n                    if cache_key in self._union_cache:\n                        union = self._union_cache[cache_key]\n                    else:\n                        union = set1.union(set2)\n                        self._union_cache[cache_key] = union\n                        \n                    new_sets.add(union)\n                    \n            closure.update(new_sets)\n        \n        # Convert back to tuples for easier handling\n        return [tuple(sorted(s)) for s in closure]\n    \n    def calculate_element_frequencies(self, family):\n        \"\"\"Calculate the frequency of each element in the family.\"\"\"\n        # Check cache\n        family_key = tuple(sorted(family))\n        if family_key in self._frequency_cache:\n            return self._frequency_cache[family_key]\n            \n        element_counts = Counter()\n        for s in family:\n            element_counts.update(s)\n        \n        family_size = max(1, len(family))  # Avoid division by zero\n        frequencies = {element: count / family_size for element, count in element_counts.items()}\n        \n        # Update cache\n        self._frequency_cache[family_key] = frequencies\n        return frequencies\n    \n    def evaluate_family(self, family):\n        \"\"\"Evaluate a family using maximum element frequency for the Frankl conjecture.\n        A counterexample is any family where max frequency < 0.5.\"\"\"\n        if not family:\n            return 1.0\n        \n        frequencies = self.calculate_element_frequencies(family)\n        if not frequencies:\n            return 1.0\n        \n        # Use the maximum element frequency as the sole evaluation metric\n        max_frequency = max(frequencies.values()) if frequencies else 1.0\n        \n        # Store metrics for analysis purposes\n        metrics = {\n            'max_frequency': max_frequency,\n            # These are still calculated for analysis but not used in evaluation\n            'min_k_avg': self._get_min_k_average(frequencies, k=3),\n            'below_threshold_ratio': self._get_below_threshold_ratio(frequencies),\n            'frequency_variance': self._get_frequency_variance(frequencies),\n            'structure_complexity': self._get_structure_complexity(family, frequencies)\n        }\n        \n        # Store the component metrics for analysis\n        family_key = tuple(sorted(family))\n        self._metrics_cache = self._metrics_cache if hasattr(self, '_metrics_cache') else {}\n        self._metrics_cache[family_key] = metrics\n        \n        # Add a small penalty for very large families to encourage diversity in the population\n        size_penalty = 0.0001 * (len(family) / 100)\n        \n        # Return the maximum frequency as the score (lower is better)\n        return max_frequency + size_penalty\n        \n    def _get_min_k_average(self, frequencies, k=3):\n        \"\"\"Calculate the average of the k lowest element frequencies.\"\"\"\n        if not frequencies:\n            return 1.0\n            \n        sorted_freqs = sorted(frequencies.values())\n        k = min(k, len(sorted_freqs))\n        \n        if k == 0:\n            return 1.0\n            \n        return sum(sorted_freqs[:k]) / k\n        \n    def _get_below_threshold_ratio(self, frequencies, threshold=0.5):\n        \"\"\"Calculate the ratio of elements below the threshold frequency.\"\"\"\n        if not frequencies:\n            return 0.0\n            \n        below_count = sum(1 for freq in frequencies.values() if freq < threshold)\n        return 1.0 - (below_count / max(1, len(frequencies)))\n        \n    def _get_frequency_variance(self, frequencies):\n        \"\"\"Calculate the variance in element frequencies.\"\"\"\n        if not frequencies or len(frequencies) < 2:\n            return 0.0\n            \n        freqs = list(frequencies.values())\n        mean = sum(freqs) / len(freqs)\n        variance = sum((f - mean) ** 2 for f in freqs) / len(freqs)\n        \n        # Normalize to [0,1] range with a reasonable assumption about max variance\n        return min(1.0, variance * 4)\n        \n    def _get_structure_complexity(self, family, frequencies):\n        \"\"\"Evaluate the structural complexity of the family.\"\"\"\n        if not family or not frequencies:\n            return 1.0\n            \n        # Consider the distribution of set sizes\n        set_sizes = [len(s) for s in family]\n        size_variety = len(set(set_sizes)) / max(1, len(family))\n        \n        # Consider the coverage of the ground set\n        ground_set_coverage = len(frequencies) / max(1, self.ground_set_size)\n        \n        # Consider the balance between family size and ground set size\n        size_ratio = min(1.0, len(family) / (2 ** min(10, len(frequencies))))\n        \n        # Combine these factors (lower is better for our search)\n        return 0.4 * (1 - size_variety) + 0.3 * (1 - ground_set_coverage) + 0.3 * size_ratio\n    \n    def evaluate_population(self, population):\n        \"\"\"Evaluate all families in the population in parallel.\"\"\"\n        if self.use_multiprocessing and len(population) > 10:\n            with mp.Pool(processes=mp.cpu_count()) as pool:\n                results = pool.map(self._evaluate_single, population)\n            return results\n        else:\n            return [self._evaluate_single(seed_sets) for seed_sets in population]\n    \n    def _evaluate_single(self, seed_sets):\n        \"\"\"Helper function to evaluate a single set of seed sets.\"\"\"\n        family = self.get_union_closure(seed_sets)\n        \n        # Only consider non-empty, union-closed families\n        if family and self.is_union_closed(family):\n            max_frequency = self.evaluate_family(family)\n            return (seed_sets, family, max_frequency)\n        else:\n            return (seed_sets, [], 1.0)  # Invalid family\n    \n    def mutate_seed_sets(self, seed_sets, mutation_rate=None):\n        \"\"\"Apply mutations to seed sets to explore the space.\"\"\"\n        if mutation_rate is None:\n            mutation_rate = self.mutation_rate\n            \n        new_seed_sets = []\n        \n        # Adaptive mutation: higher rate for stagnant searches\n        if len(self.history['best_frequencies']) > 100:\n            if len(set(self.history['best_frequencies'][-100:])) < 5:\n                mutation_rate *= 1.5  # Increase mutation if stuck\n                \n        # Operations: add/remove sets, add/remove elements\n        for s in seed_sets:\n            if np.random.random() < mutation_rate:\n                # Mutate this set\n                s_list = list(s)\n                \n                # Choose a mutation type\n                mutation_type = np.random.choice(['add', 'remove', 'swap'])\n                \n                if mutation_type == 'add' or (not s_list and mutation_type == 'swap'):\n                    # Add an element\n                    new_element = np.random.randint(1, self.ground_set_size + 1)\n                    if new_element not in s_list:\n                        s_list.append(new_element)\n                        s_list.sort()\n                elif mutation_type == 'remove' and s_list:\n                    # Remove an element\n                    idx_to_remove = np.random.randint(0, len(s_list))\n                    s_list.pop(idx_to_remove)\n                elif mutation_type == 'swap' and s_list:\n                    # Swap an element\n                    idx_to_swap = np.random.randint(0, len(s_list))\n                    current = s_list[idx_to_swap]\n                    new_element = current\n                    while new_element == current:\n                        new_element = np.random.randint(1, self.ground_set_size + 1)\n                    s_list[idx_to_swap] = new_element\n                    s_list.sort()\n                \n                new_seed_sets.append(tuple(s_list))\n            else:\n                new_seed_sets.append(s)\n        \n        # Perform set-level mutations\n        set_mutation_type = np.random.choice(['add', 'remove', 'merge', 'none'], \n                                           p=[0.3, 0.3, 0.2, 0.2])\n        \n        if set_mutation_type == 'add':\n            # Add a set\n            set_size = np.random.randint(1, min(5, self.ground_set_size) + 1)\n            new_set = tuple(sorted(np.random.choice(\n                range(1, self.ground_set_size + 1), \n                size=min(set_size, self.ground_set_size),\n                replace=False)))\n            if new_set and new_set not in new_seed_sets:\n                new_seed_sets.append(new_set)\n        elif set_mutation_type == 'remove' and len(new_seed_sets) > 1:\n            # Remove a set\n            idx_to_remove = np.random.randint(0, len(new_seed_sets))\n            new_seed_sets.pop(idx_to_remove)\n        elif set_mutation_type == 'merge' and len(new_seed_sets) >= 2:\n            # Merge two sets\n            idx1, idx2 = np.random.choice(len(new_seed_sets), size=2, replace=False)\n            set1 = set(new_seed_sets[idx1])\n            set2 = set(new_seed_sets[idx2])\n            merged = tuple(sorted(set1.union(set2)))\n            \n            # Replace both sets with the merged one\n            if idx1 > idx2:\n                new_seed_sets.pop(idx1)\n                new_seed_sets.pop(idx2)\n            else:\n                new_seed_sets.pop(idx2)\n                new_seed_sets.pop(idx1)\n                \n            if merged:\n                new_seed_sets.append(merged)\n        \n        # Ensure we have at least one set\n        if not new_seed_sets:\n            new_element = np.random.randint(1, self.ground_set_size + 1)\n            new_seed_sets.append((new_element,))\n            \n        return new_seed_sets\n    \n    def crossover(self, parent1, parent2):\n        \"\"\"Perform crossover between two families' seed sets.\"\"\"\n        if not parent1 or not parent2:\n            return parent1 if parent1 else parent2\n            \n        # Choose crossover type\n        crossover_type = np.random.choice(['one_point', 'two_point', 'uniform'])\n        \n        if crossover_type == 'one_point':\n            # One-point crossover\n            cut_point = np.random.randint(0, min(len(parent1), len(parent2)) + 1)\n            child = parent1[:cut_point] + parent2[cut_point:]\n        elif crossover_type == 'two_point' and min(len(parent1), len(parent2)) > 2:\n            # Two-point crossover\n            points = sorted(np.random.choice(range(min(len(parent1), len(parent2)) + 1), \n                                          size=2, replace=False))\n            child = parent1[:points[0]] + parent2[points[0]:points[1]] + parent1[points[1]:]\n        else:\n            # Uniform crossover\n            child = []\n            for i in range(max(len(parent1), len(parent2))):\n                if i < len(parent1) and i < len(parent2):\n                    # Choose from either parent\n                    child.append(parent1[i] if np.random.random() < 0.5 else parent2[i])\n                elif i < len(parent1):\n                    # Only parent1 has this index\n                    if np.random.random() < 0.5:  # 50% chance to include\n                        child.append(parent1[i])\n                else:\n                    # Only parent2 has this index\n                    if np.random.random() < 0.5:  # 50% chance to include\n                        child.append(parent2[i])\n        \n        # Remove duplicates while preserving order\n        seen = set()\n        unique_child = []\n        for item in child:\n            if item not in seen:\n                seen.add(item)\n                unique_child.append(item)\n                \n        # Ensure we have at least one set\n        if not unique_child:\n            if parent1:\n                unique_child.append(parent1[0])\n            elif parent2:\n                unique_child.append(parent2[0])\n            else:\n                new_element = np.random.randint(1, self.ground_set_size + 1)\n                unique_child.append((new_element,))\n                \n        return unique_child\n    \n    def calculate_population_diversity(self, population):\n        \"\"\"Calculate diversity metric for the population.\"\"\"\n        if not population:\n            return 0\n            \n        # Use a simple measure: the average pairwise difference between individuals\n        total_diff = 0\n        count = 0\n        \n        # Sample pairs to avoid O(n²) computation for large populations\n        sample_size = min(100, len(population) * (len(population) - 1) // 2)\n        pairs = np.random.choice(len(population), size=(sample_size, 2), replace=True)\n        \n        for i, j in pairs:\n            if i != j:\n                # Count the number of different sets\n                set1 = set(population[i])\n                set2 = set(population[j])\n                diff = len(set1.symmetric_difference(set2)) / max(1, len(set1) + len(set2))\n                total_diff += diff\n                count += 1\n                \n        return total_diff / max(1, count)\n    \n    def search(self):\n        \"\"\"Use a genetic algorithm approach to search for families with max element frequency < 0.5.\"\"\"\n        start_time = time.time()\n        no_improvement_count = 0\n        \n        # Initialize population with random seed sets\n        population = []\n        for _ in range(self.population_size):\n            num_sets = np.random.randint(2, min(8, self.ground_set_size // 2))\n            seed_sets = []\n            for _ in range(num_sets):\n                set_size = np.random.randint(1, min(5, self.ground_set_size) + 1)\n                new_set = tuple(sorted(np.random.choice(\n                    range(1, self.ground_set_size + 1), \n                    size=min(set_size, self.ground_set_size),\n                    replace=False)))\n                if new_set:  # Ensure non-empty\n                    seed_sets.append(new_set)\n            if not seed_sets:  # Ensure at least one set\n                new_element = np.random.randint(1, self.ground_set_size + 1)\n                seed_sets.append((new_element,))\n            population.append(seed_sets)\n        \n        # Create a progress bar if verbose\n        iterator = tqdm(range(self.max_iterations)) if self.verbose else range(self.max_iterations)\n        \n        for iteration in iterator:\n            # Evaluate population\n            evaluation_results = self.evaluate_population(population)\n            \n            # Extract frequencies and update best solution\n            frequencies = [freq for _, _, freq in evaluation_results]\n            seed_sets_list = [seeds for seeds, _, _ in evaluation_results]\n            family_list = [family for _, family, _ in evaluation_results]\n            \n            # Track metrics\n            avg_frequency = sum(frequencies) / len(frequencies)\n            self.history['avg_frequencies'].append(avg_frequency)\n            \n            # Find the best solution in this generation\n            min_idx = np.argmin(frequencies)\n            current_best_freq = frequencies[min_idx]\n            current_best_family = family_list[min_idx]\n            \n            if current_best_freq < self.best_max_frequency and current_best_family:\n                self.best_max_frequency = current_best_freq\n                self.best_family = current_best_family\n                no_improvement_count = 0\n                \n                if self.verbose:\n                    print(f\"\\nIteration {iteration}, New best: {current_best_freq:.4f}\")\n                    print(f\"Family size: {len(self.best_family)}\")\n                    element_freqs = self.calculate_element_frequencies(self.best_family)\n                    max_element = max(element_freqs.items(), key=lambda x: x[1])\n                    print(f\"Max frequency element: {max_element[0]} with frequency {max_element[1]:.4f}\")\n                    print(f\"Distance from counterexample threshold: {(max_element[1] - 0.5) * 100:.2f}%\")\n                    \n                # If we find a counterexample, stop immediately and report it\n                if current_best_freq < 0.5:\n                    print(\"\\n========================================\")\n                    print(\"COUNTEREXAMPLE TO FRANKL'S CONJECTURE FOUND!\")\n                    print(\"========================================\")\n                    print(f\"Maximum element frequency: {current_best_freq:.6f}\")\n                    print(f\"Family size: {len(self.best_family)}\")\n                    print(f\"Ground set size: {self.ground_set_size}\")\n                    \n                    # Display the actual counterexample\n                    print(\"\\nCounterexample family:\")\n                    for i, s in enumerate(sorted(self.best_family, key=len)):\n                        print(f\"Set {i+1}: {s}\")\n                        \n                    # Print the element frequencies\n                    element_freqs = self.calculate_element_frequencies(self.best_family)\n                    print(\"\\nElement frequencies:\")\n                    for element, freq in sorted(element_freqs.items(), key=lambda x: x[1], reverse=True):\n                        print(f\"Element {element}: {freq:.6f}\")\n                        \n                    return self.best_family\n            else:\n                no_improvement_count += 1\n            \n            # Record history\n            self.history['best_frequencies'].append(self.best_max_frequency)\n            \n            if self.best_family:\n                self.history['family_sizes'].append(len(self.best_family))\n            else:\n                self.history['family_sizes'].append(0)\n                \n            diversity = self.calculate_population_diversity(seed_sets_list)\n            self.history['diversity'].append(diversity)\n            \n            # Check for early stopping\n            if no_improvement_count >= self.early_stopping:\n                if self.verbose:\n                    print(f\"\\nEarly stopping after {no_improvement_count} iterations without improvement\")\n                break\n                \n            # Create new population\n            new_population = []\n            \n            # Elitism: keep the best solutions\n            elite_count = max(1, self.population_size // 20)\n            elite_indices = np.argsort(frequencies)[:elite_count]\n            for idx in elite_indices:\n                new_population.append(seed_sets_list[idx])\n            \n            # Fill the rest with tournament selection and crossover\n            while len(new_population) < self.population_size:\n                # Tournament selection\n                tournament = np.random.choice(len(population), self.tournament_size, replace=False)\n                tournament_freqs = [frequencies[i] for i in tournament]\n                \n                # Select the best two parents\n                parent_indices = np.argsort(tournament_freqs)[:2]\n                parent1_idx = tournament[parent_indices[0]]\n                parent2_idx = tournament[parent_indices[1]]\n                \n                parent1 = seed_sets_list[parent1_idx]\n                parent2 = seed_sets_list[parent2_idx]\n                \n                # Crossover with probability\n                if np.random.random() < self.crossover_rate:\n                    child = self.crossover(parent1, parent2)\n                else:\n                    child = parent1.copy() if np.random.random() < 0.5 else parent2.copy()\n                \n                # Mutation\n                child = self.mutate_seed_sets(child)\n                \n                new_population.append(child)\n            \n            population = new_population\n            \n            # Adaptive parameter adjustment\n            if iteration > 100 and iteration % 50 == 0:\n                # If diversity is low, increase mutation rate\n                if self.history['diversity'][-1] < 0.1:\n                    self.mutation_rate = min(0.8, self.mutation_rate * 1.2)\n                # If progress is stagnant but diversity is high, increase selection pressure\n                elif len(set(self.history['best_frequencies'][-50:])) < 3:\n                    self.tournament_size = min(10, self.tournament_size + 1)\n                    \n            # Update progress bar with status\n            if self.verbose and isinstance(iterator, tqdm):\n                iterator.set_description(f\"Best: {self.best_max_frequency:.4f}, Avg: {avg_frequency:.4f}\")\n        \n        # Display final statistics\n        elapsed_time = time.time() - start_time\n        if self.verbose:\n            print(f\"\\nSearch completed in {elapsed_time:.2f} seconds\")\n            print(f\"Total iterations: {min(iteration + 1, self.max_iterations)}\")\n            print(f\"Best frequency found: {self.best_max_frequency:.4f}\")\n            \n        # Return the best family found\n        return self.best_family\n    \n    def plot_results(self):\n        \"\"\"Plot comprehensive search history and analysis.\"\"\"\n        if not self.history['best_frequencies']:\n            print(\"No search history to plot.\")\n            return\n            \n        plt.figure(figsize=(16, 12))\n        \n        # 1. Plot metrics evolution\n        plt.subplot(3, 2, 1)\n        plt.plot(self.history['best_frequencies'], label='Best Score', color='blue')\n        plt.plot(self.history['avg_frequencies'], label='Avg Score', color='lightblue', alpha=0.7)\n        plt.axhline(y=0.5, color='r', linestyle='--', label='Conjecture Threshold')\n        plt.xlabel('Iteration')\n        plt.ylabel('Score')\n        plt.legend()\n        plt.title('Search Progress')\n        \n        # 2. Plot family size evolution\n        plt.subplot(3, 2, 2)\n        plt.plot(self.history['family_sizes'], color='green')\n        plt.xlabel('Iteration')\n        plt.ylabel('Family Size')\n        plt.title('Best Family Size Evolution')\n        \n        # 3. Plot population diversity\n        plt.subplot(3, 2, 3)\n        plt.plot(self.history['diversity'], color='purple')\n        plt.xlabel('Iteration')\n        plt.ylabel('Diversity')\n        plt.title('Population Diversity')\n        \n        # 4. Element frequency distribution for the best family\n        if self.best_family:\n            plt.subplot(3, 2, 4)\n            frequencies = self.calculate_element_frequencies(self.best_family)\n            \n            # Create histogram of frequencies\n            freq_values = list(frequencies.values())\n            plt.hist(freq_values, bins=20, color='teal', alpha=0.7)\n            plt.axvline(x=0.5, color='r', linestyle='--', label='Threshold')\n            plt.xlabel('Frequency')\n            plt.ylabel('Count')\n            plt.title('Distribution of Element Frequencies')\n            plt.legend()\n            \n            # 5. Set size distribution\n            plt.subplot(3, 2, 5)\n            set_sizes = [len(s) for s in self.best_family]\n            size_counts = Counter(set_sizes)\n            sizes = sorted(size_counts.keys())\n            counts = [size_counts[s] for s in sizes]\n            \n            plt.bar(sizes, counts, color='orange')\n            plt.xlabel('Set Size')\n            plt.ylabel('Count')\n            plt.title('Set Size Distribution')\n            \n            # 6. Sorted element frequencies\n            plt.subplot(3, 2, 6)\n            sorted_elements = sorted(frequencies.items(), key=lambda x: x[1])\n            elements = [e for e, _ in sorted_elements]\n            freqs = [f for _, f in sorted_elements]\n            \n            plt.scatter(range(len(elements)), freqs, color='blue', alpha=0.7)\n            plt.axhline(y=0.5, color='r', linestyle='--')\n            plt.xlabel('Element Index (sorted by frequency)')\n            plt.ylabel('Frequency')\n            plt.title('Sorted Element Frequencies')\n            \n            # Add text annotation for the maximum frequency\n            max_freq = max(freqs)\n            plt.annotate(f'Max: {max_freq:.4f}', \n                         xy=(len(freqs)-1, max_freq),\n                         xytext=(len(freqs)-len(freqs)//4, max_freq+0.1),\n                         arrowprops=dict(facecolor='black', shrink=0.05, width=1.5))\n        \n        plt.tight_layout()\n        \n        # Create a second figure for additional analysis\n        if self.best_family and len(self.best_family) > 0:\n            plt.figure(figsize=(16, 8))\n            \n            # 1. Heatmap of set membership\n            plt.subplot(1, 2, 1)\n            frequencies = self.calculate_element_frequencies(self.best_family)\n            elements = sorted(frequencies.keys())\n            \n            # Create matrix: rows=sets, cols=elements\n            matrix = np.zeros((min(50, len(self.best_family)), len(elements)))\n            for i, s in enumerate(self.best_family[:50]):  # Limit to 50 sets for visibility\n                for j, e in enumerate(elements):\n                    matrix[i, j] = 1 if e in s else 0\n            \n            # Plot heatmap\n            sns.heatmap(matrix, cmap='Blues', cbar=False)\n            plt.xlabel('Elements')\n            plt.ylabel('Sets (first 50)')\n            plt.title('Set Membership Matrix')\n            \n            # 2. Network visualization of element co-occurrence\n            plt.subplot(1, 2, 2)\n            \n            # Count co-occurrences\n            co_occurrences = np.zeros((len(elements), len(elements)))\n            for s in self.best_family:\n                for i, e1 in enumerate(elements):\n                    for j, e2 in enumerate(elements):\n                        if e1 in s and e2 in s:\n                            co_occurrences[i, j] += 1\n            \n            # Normalize and visualize\n            co_occurrences = co_occurrences / len(self.best_family)\n            mask = np.triu(np.ones_like(co_occurrences, dtype=bool))\n            sns.heatmap(co_occurrences, mask=mask, cmap=\"YlGnBu\", vmin=0, vmax=1)\n            plt.title('Element Co-occurrence')\n            \n            plt.tight_layout()\n        \n        plt.show()\n    \n    def analyze_best_family(self):\n        \"\"\"Analyze the best family found with comprehensive metrics.\"\"\"\n        if not self.best_family:\n            print(\"No family found.\")\n            return\n            \n        print(\"\\nBest Family Analysis:\")\n        print(f\"Family size: {len(self.best_family)}\")\n        \n        # Analyze element frequencies\n        frequencies = self.calculate_element_frequencies(self.best_family)\n        max_freq = max(frequencies.values()) if frequencies else 0\n        print(f\"Maximum element frequency: {max_freq:.4f}\")\n        \n        # Calculate all metrics\n        family_key = tuple(sorted(self.best_family))\n        if hasattr(self, '_metrics_cache') and family_key in self._metrics_cache:\n            metrics = self._metrics_cache[family_key]\n        else:\n            metrics = {\n                'min_k_avg': self._get_min_k_average(frequencies, k=3),\n                'below_threshold_ratio': self._get_below_threshold_ratio(frequencies),\n                'frequency_variance': self._get_frequency_variance(frequencies),\n                'structure_complexity': self._get_structure_complexity(self.best_family, frequencies),\n                'max_frequency': max_freq\n            }\n        \n        print(\"\\nMetrics Summary:\")\n        print(f\"Average of 3 lowest frequencies: {metrics['min_k_avg']:.4f}\")\n        below_count = sum(1 for freq in frequencies.values() if freq < 0.5)\n        print(f\"Elements below 0.5 threshold: {below_count}/{len(frequencies)} ({(1-metrics['below_threshold_ratio'])*100:.1f}%)\")\n        print(f\"Frequency variance: {metrics['frequency_variance']:.4f}\")\n        print(f\"Structure complexity score: {metrics['structure_complexity']:.4f}\")\n        \n        # Frequency distribution visualization\n        print(\"\\nElement frequencies:\")\n        sorted_freqs = sorted(frequencies.items(), key=lambda x: x[1])\n        \n        # Print a simple histogram of frequencies\n        freq_ranges = {'0.0-0.1': 0, '0.1-0.2': 0, '0.2-0.3': 0, '0.3-0.4': 0, \n                      '0.4-0.5': 0, '0.5-0.6': 0, '0.6-0.7': 0, '0.7-0.8': 0, \n                      '0.8-0.9': 0, '0.9-1.0': 0}\n        \n        for _, freq in frequencies.items():\n            range_idx = min(int(freq * 10), 9)\n            range_key = f\"{range_idx/10:.1f}-{(range_idx+1)/10:.1f}\"\n            freq_ranges[range_key] += 1\n            \n        print(\"Frequency distribution:\")\n        for range_key, count in freq_ranges.items():\n            bar = \"#\" * (count * 50 // max(1, len(frequencies)))\n            print(f\"{range_key}: {count} {bar}\")\n        \n        # Detailed top and bottom elements\n        print(\"\\nTop 5 highest frequency elements:\")\n        for element, freq in sorted(frequencies.items(), key=lambda x: x[1], reverse=True)[:5]:\n            print(f\"Element {element}: {freq:.4f}\")\n            \n        print(\"\\nTop 5 lowest frequency elements:\")\n        for element, freq in sorted_freqs[:5]:\n            print(f\"Element {element}: {freq:.4f}\")\n            \n        # Analyze set sizes\n        set_sizes = [len(s) for s in self.best_family]\n        size_counts = Counter(set_sizes)\n        print(\"\\nSet size distribution:\")\n        for size, count in sorted(size_counts.items()):\n            bar = \"*\" * (count * 50 // len(self.best_family))\n            print(f\"Size {size}: {count} sets ({count/len(self.best_family)*100:.1f}%) {bar}\")\n            \n        # Find minimal generating sets\n        min_generating_size = float('inf')\n        \n        # Start with small random samples\n        for sample_size in range(1, min(len(self.best_family), 10)):\n            for _ in range(100):  # Try multiple random samples\n                sample = list(np.random.choice(len(self.best_family), sample_size, replace=False))\n                sample_sets = [self.best_family[i] for i in sample]\n                closure = self.get_union_closure(sample_sets)\n                \n                # Check if this sample generates the whole family\n                if set(tuple(sorted(s)) for s in closure) == set(tuple(sorted(s)) for s in self.best_family):\n                    min_generating_size = min(min_generating_size, sample_size)\n                    break\n            \n            if min_generating_size <= sample_size:\n                break\n                \n        print(f\"\\nMinimum generating set size (estimate): {min_generating_size}\")\n        \n        # Analyze elements that appear in most of the smaller sets\n        small_sets = [s for s in self.best_family if len(s) <= 3]\n        if small_sets:\n            small_set_elements = Counter()\n            for s in small_sets:\n                small_set_elements.update(s)\n            \n            print(\"\\nMost common elements in small sets (size ≤ 3):\")\n            for element, count in small_set_elements.most_common(5):\n                print(f\"Element {element}: appears in {count}/{len(small_sets)} small sets ({count/len(small_sets)*100:.1f}%)\")\n        \n        # Check if this is close to a counterexample\n        threshold = 0.5\n        elements_below = [element for element, freq in frequencies.items() if freq < threshold]\n        if elements_below:\n            print(f\"\\nElements below {threshold} threshold: {len(elements_below)}/{len(frequencies)}\")\n            if max_freq < 0.55:\n                print(\"\\nThis family is close to a potential counterexample!\")\n                print(f\"The gap to the conjecture threshold is only {(max_freq - 0.5) * 100:.2f}%\")\n                \n                # Analyze which sets contain the elements with lowest frequencies\n                low_elements = [element for element, freq in sorted_freqs[:3]]\n                print(\"\\nAnalyzing sets containing the lowest frequency elements:\")\n                for element in low_elements:\n                    containing_sets = [s for s in self.best_family if element in s]\n                    print(f\"Element {element} (freq: {frequencies[element]:.4f}) appears in {len(containing_sets)}/{len(self.best_family)} sets\")\n                    \n                    # Show some example sets\n                    if containing_sets:\n                        print(\"Example sets:\")\n                        for i, s in enumerate(containing_sets[:3]):\n                            print(f\"  Set {i+1}: {s}\")\n                        if len(containing_sets) > 3:\n                            print(f\"  ... and {len(containing_sets)-3} more sets\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T03:30:27.326275Z","iopub.execute_input":"2025-03-15T03:30:27.327429Z","iopub.status.idle":"2025-03-15T03:30:27.634852Z","shell.execute_reply.started":"2025-03-15T03:30:27.327351Z","shell.execute_reply":"2025-03-15T03:30:27.633464Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def deep_search(ground_size=16, iterations=15000, population_size=500, use_multiprocessing=True, verbose=True):\n    \"\"\"Run a deeper search with more resources for a specific ground set size.\"\"\"\n    print(f\"\\n=== Deep search with ground set size {ground_size} ===\")\n    print(f\"Parameters: iterations={iterations}, population_size={population_size}\")\n    \n    searcher = FranklAdversarialSearch(\n        ground_set_size=ground_size,\n        max_iterations=iterations,\n        population_size=population_size,\n        tournament_size=10,                # Increased tournament size for higher selection pressure\n        mutation_rate=0.3,\n        crossover_rate=0.8,\n        early_stopping=2000,               # Increased early stopping threshold for thorough search\n        use_multiprocessing=use_multiprocessing,\n        verbose=verbose\n    )\n    \n    result = searcher.search()\n    \n    print(f\"\\nDeep search completed.\")\n    if searcher.best_max_frequency < 0.5:\n        print(\"COUNTEREXAMPLE TO FRANKL'S CONJECTURE FOUND!\")\n    else:\n        print(\"No counterexample found. This supports the conjecture.\")\n        print(f\"Best maximum element frequency found: {searcher.best_max_frequency:.6f}\")\n        print(f\"Gap to counterexample threshold: {(searcher.best_max_frequency - 0.5) * 100:.4f}%\")\n    \n    # Perform detailed analysis\n    print(\"\\nPerforming detailed analysis of the best family found...\")\n    searcher.analyze_best_family()\n    searcher.plot_results()\n    \n    return searcher","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T03:30:27.636608Z","iopub.execute_input":"2025-03-15T03:30:27.637039Z","iopub.status.idle":"2025-03-15T03:30:27.657949Z","shell.execute_reply.started":"2025-03-15T03:30:27.636997Z","shell.execute_reply":"2025-03-15T03:30:27.656670Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def random_search(ground_set_size=10, num_attempts=1000, max_family_size=50, verbose=True):\n    \"\"\"\n    Perform a random search for counterexamples to Frankl's conjecture.\n    \n    This function generates random union-closed families and checks if they\n    violate the conjecture (have maximum element frequency < 0.5).\n    \n    Args:\n        ground_set_size: Size of the ground set\n        num_attempts: Number of random families to generate\n        max_family_size: Maximum number of seed sets to generate for each family\n        verbose: Whether to display progress information\n    \n    Returns:\n        The best family found (closest to being a counterexample)\n    \"\"\"\n    print(f\"\\n=== Random search with ground set size {ground_set_size} ===\")\n    print(f\"Parameters: attempts={num_attempts}, max_family_size={max_family_size}\")\n    \n    best_family = None\n    best_max_frequency = 1.0\n    \n    # For calculating element frequencies\n    def calculate_frequencies(family):\n        if not family:\n            return {}\n        \n        element_counts = Counter()\n        for s in family:\n            element_counts.update(s)\n        \n        family_size = len(family)\n        return {element: count / family_size for element, count in element_counts.items()}\n    \n    # For generating random sets\n    def generate_random_set():\n        set_size = np.random.randint(1, ground_set_size + 1)\n        return tuple(sorted(np.random.choice(range(1, ground_set_size + 1), \n                                          size=min(set_size, ground_set_size),\n                                          replace=False)))\n    \n    # Progress tracking\n    if verbose:\n        iterator = tqdm(range(num_attempts))\n    else:\n        iterator = range(num_attempts)\n    \n    for attempt in iterator:\n        # Generate a random number of seed sets\n        num_sets = np.random.randint(1, max_family_size + 1)\n        seed_sets = []\n        \n        # Generate random sets\n        for _ in range(num_sets):\n            new_set = generate_random_set()\n            if new_set:  # Non-empty\n                seed_sets.append(new_set)\n        \n        # Skip if no sets were generated\n        if not seed_sets:\n            continue\n            \n        # Generate the union closure\n        closure = set()\n        for s in seed_sets:\n            closure.add(frozenset(s))\n            \n        old_size = 0\n        while old_size != len(closure):\n            old_size = len(closure)\n            new_sets = set()\n            for set1, set2 in itertools.combinations(closure, 2):\n                new_sets.add(frozenset(set1.union(set2)))\n            closure.update(new_sets)\n        \n        # Convert back to tuples for easier handling\n        family = [tuple(sorted(s)) for s in closure]\n        \n        # Calculate maximum element frequency\n        frequencies = calculate_frequencies(family)\n        max_freq = max(frequencies.values()) if frequencies else 1.0\n        \n        # Update if this is better than our previous best\n        if max_freq < best_max_frequency:\n            best_max_frequency = max_freq\n            best_family = family\n            \n            if verbose:\n                print(f\"\\nAttempt {attempt}, New best: {max_freq:.4f}\")\n                print(f\"Family size: {len(family)}\")\n                max_element = max(frequencies.items(), key=lambda x: x[1]) if frequencies else None\n                if max_element:\n                    print(f\"Max frequency element: {max_element[0]} with frequency {max_element[1]:.4f}\")\n                print(f\"Distance from counterexample threshold: {(max_freq - 0.5) * 100:.2f}%\")\n                \n            # If a counterexample is found, stop immediately\n            if max_freq < 0.5:\n                print(\"\\n========================================\")\n                print(\"COUNTEREXAMPLE TO FRANKL'S CONJECTURE FOUND!\")\n                print(\"========================================\")\n                print(f\"Maximum element frequency: {max_freq:.6f}\")\n                print(f\"Family size: {len(family)}\")\n                print(f\"Ground set size: {ground_set_size}\")\n                break\n                \n        # Update progress bar\n        if verbose and isinstance(iterator, tqdm):\n            iterator.set_description(f\"Best: {best_max_frequency:.4f}\")\n    \n    # Final report\n    print(\"\\nRandom search completed.\")\n    print(f\"Best maximum element frequency found: {best_max_frequency:.6f}\")\n    print(f\"Family size: {len(best_family) if best_family else 0}\")\n    print(f\"Gap to counterexample threshold: {(best_max_frequency - 0.5) * 100:.4f}%\")\n    \n    # Analyze the best family\n    if best_family:\n        # Create a simple analysis of the best family\n        frequencies = calculate_frequencies(best_family)\n        print(\"\\nElement frequencies:\")\n        for element, freq in sorted(frequencies.items(), key=lambda x: x[1], reverse=True)[:10]:\n            print(f\"Element {element}: {freq:.4f}\")\n        \n        # Set size distribution\n        set_sizes = [len(s) for s in best_family]\n        size_counts = Counter(set_sizes)\n        print(\"\\nSet size distribution:\")\n        for size, count in sorted(size_counts.items()):\n            print(f\"Size {size}: {count} sets ({count/len(best_family)*100:.1f}%)\")\n    \n    return best_family","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T03:30:27.659599Z","iopub.execute_input":"2025-03-15T03:30:27.659877Z","iopub.status.idle":"2025-03-15T03:30:27.677367Z","shell.execute_reply.started":"2025-03-15T03:30:27.659854Z","shell.execute_reply":"2025-03-15T03:30:27.676054Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Example usage with random search added:\nif __name__ == \"__main__\":\n    # Choose a search method:\n    \n    # Option 1: Single search with specific parameters\n    # searcher = run_frankl_search(ground_set_size=12, iterations=3000, population_size=300)\n    \n    # Option 2: Systematic search with increasing ground set sizes\n    # best_results = systematic_search(ground_sizes=[8, 12, 16], verbose=True)\n    \n    # Option 3: Deep search on a specific ground set size\n     searcher = deep_search(ground_size=70, iterations=10000, population_size=500)\n    \n    # Option 4: Random search approach\n    #best_family = random_search(ground_set_size=70, num_attempts=2000, max_family_size=20, verbose=True)\n    \n    # If you want to verify a specific family\n    # family = [(1, 2), (2, 3), (1, 3), (1, 2, 3)]  # Example family\n    # verify_family(family, ground_set_size=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T03:45:06.919159Z","iopub.execute_input":"2025-03-15T03:45:06.919575Z"}},"outputs":[{"name":"stdout","text":"\n=== Deep search with ground set size 70 ===\nParameters: iterations=10000, population_size=500\n","output_type":"stream"},{"name":"stderr","text":"Best: 0.5041, Avg: 0.6985:   0%|          | 1/10000 [00:09<25:43:51,  9.26s/it]","output_type":"stream"},{"name":"stdout","text":"\nIteration 0, New best: 0.5041\nFamily size: 127\nMax frequency element: 8 with frequency 0.5039\nDistance from counterexample threshold: 0.39%\n","output_type":"stream"},{"name":"stderr","text":"Best: 0.5000, Avg: 0.6450:   0%|          | 2/10000 [00:14<18:24:42,  6.63s/it]","output_type":"stream"},{"name":"stdout","text":"\nIteration 1, New best: 0.5000\nFamily size: 2\nMax frequency element: 11 with frequency 0.5000\nDistance from counterexample threshold: 0.00%\n","output_type":"stream"},{"name":"stderr","text":"Best: 0.5000, Avg: 0.7149:   9%|▉         | 931/10000 [04:48<46:03,  3.28it/s]  ","output_type":"stream"}],"execution_count":null}]}